{"meta":{"title":"天律界","subtitle":"精诚所至，金石为开","description":"天律界中子的Github page","author":"天律界中子","url":"http://yoursite.com"},"pages":[{"title":"","date":"2018-04-29T13:47:05.629Z","updated":"2018-04-29T13:47:05.629Z","comments":true,"path":"About/index.html","permalink":"http://yoursite.com/About/index.html","excerpt":"","text":"简介图像处理算法工程师一枚，以“天律界中子”的昵称混迹于网络江湖。 教育经历 2009-2013 南昌大学"},{"title":"","date":"2018-04-28T14:23:30.645Z","updated":"2018-04-28T14:23:30.645Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2018-04-28T14:24:22.605Z","updated":"2018-04-28T14:24:22.605Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"YUV420中的stride","slug":"stride-in-yuv420","date":"2018-08-21T13:50:14.000Z","updated":"2018-08-21T14:07:43.915Z","comments":true,"path":"2018/08/21/stride-in-yuv420/","link":"","permalink":"http://yoursite.com/2018/08/21/stride-in-yuv420/","excerpt":"","text":"一幅图像除了宽度和高度，有时还有stride，详情参见Image Stride。对于RGB图像，stride很好理解。但是对于YUV图像，由于Y、U、V三个通道的宽度和排列方式不同，所以stride的样子不是很直观。最近我在工作时就遇到了这个问题。经过一番搜索，我在libyuv中找到了答案。下面就分享我找到的YUV420中的stride的答案。 YUV420一般分为四种格式：12345678910111213141516171819202122232425262728293031323334351. I420 Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y U U U U U U U U V V V V V V V V2. YV12 Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y V V V V V V V V U U U U U U U U3. NV12 Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y U V U V U V U V U V U V U V U V4. NV21 Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y V U V U V U V U V U V U V U V U 可以看到，所有四种格式的Y通道排列是相同的。对于I420和YV12格式而言，它们的U和V通道都是分开排列的，所以这两种格式称为平面格式，即YUV420p（p，即planar）。而对于NV12和NV21格式而言，它们的U和V通道是交错排列的，所以这两种格式称为半平面格式，即YUV420sp（sp，即semi-planar）。 由于平面格式和半平面格式排列的不一致，stride的表现也就不一致。下面我就平面格式和半平面格式各举一例，来说明它们的stride的样子。以“-”表示由于stride大于图像宽度而添加的额外的空白像素。需要注意的是，没有“Y”、”U”、”V”或“-”的地方不代表任何像素。即：1234Y Y - -Y Y - -U -V - 在内存中的实际排列为：1Y Y - - Y Y - - U - V - I420 平面格式有：Y stride，U stride和V stride。 1234567891011图像宽度：8图像高度：4Y stride: 16U stride: 6V stride: 8Y Y Y Y Y Y Y Y - - - - - - - -Y Y Y Y Y Y Y Y - - - - - - - -Y Y Y Y Y Y Y Y - - - - - - - -Y Y Y Y Y Y Y Y - - - - - - - -U U U U - - U U U U - -V V V V - - - - V V V V - - - - NV12 半平面格式有：Y stride，UV stride。 12345678910图像宽度：8图像高度：4Y stride: 16UV stride: 12Y Y Y Y Y Y Y Y - - - - - - - -Y Y Y Y Y Y Y Y - - - - - - - -Y Y Y Y Y Y Y Y - - - - - - - -Y Y Y Y Y Y Y Y - - - - - - - -U V U V U V U V - - - -U V U V U V U V - - - -","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/categories/图像处理/"}],"tags":[{"name":"image-processing","slug":"image-processing","permalink":"http://yoursite.com/tags/image-processing/"},{"name":"YUV","slug":"YUV","permalink":"http://yoursite.com/tags/YUV/"},{"name":"stride","slug":"stride","permalink":"http://yoursite.com/tags/stride/"}]},{"title":"一个小技巧使OpenCL不影响渲染","slug":"opencl-trick-do-not-influence-render","date":"2018-07-21T15:02:35.000Z","updated":"2018-07-21T15:35:10.419Z","comments":true,"path":"2018/07/21/opencl-trick-do-not-influence-render/","link":"","permalink":"http://yoursite.com/2018/07/21/opencl-trick-do-not-influence-render/","excerpt":"","text":"为了缩短算法在移动端的执行时间，有时我们会使用OpenCL来进行加速。OpenCL是基于命令队列的，也就是在主机端将所有要执行的命令放入队列，在设备端从队列中取出命令并执行。多数情况下，将命令放入队列只消耗极少的时间。这样一来，设备端就堆积了大量的命令需要执行，然后就会疯狂地消耗GPU资源，使得GPU资源被100%占用，造成的后果就是某些需要使用GPU进行渲染的程序严重卡顿，例如相机预览。 经过试验，我找到了一种简单的减少GPU资源占用，保证手机渲染不受影响的方法。以相机预览为例，它的刷新频率大约要保持在24Hz，那么我们就要保证在每41.7ms的时间里，其都能抢占到GPU资源进行渲染。那么我们只要保证两点就可以不影响其渲染： OpenCL每个kernel的执行时间都小于41.7ms(推荐小于30ms)； 在每41.7ms内，至少执行一次clFinish()来给予其抢占GPU资源的机会(对，技巧就是加clFinish())。 可以预见的是，由于不再占用100%的GPU资源，算法的执行时间会比原来要长。","categories":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/categories/OpenCL/"}],"tags":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/tags/OpenCL/"},{"name":"trick","slug":"trick","permalink":"http://yoursite.com/tags/trick/"},{"name":"GPU","slug":"GPU","permalink":"http://yoursite.com/tags/GPU/"},{"name":"render","slug":"render","permalink":"http://yoursite.com/tags/render/"}]},{"title":"使用Guided Filter去除彩色噪声","slug":"chroma-noise-reduction-with-guided-filter","date":"2018-07-19T15:56:27.000Z","updated":"2018-07-21T14:55:11.444Z","comments":true,"path":"2018/07/19/chroma-noise-reduction-with-guided-filter/","link":"","permalink":"http://yoursite.com/2018/07/19/chroma-noise-reduction-with-guided-filter/","excerpt":"","text":"手机在暗光环境下拍照时，由于进光量不足，拍出的照片上经常会出现严重的彩色噪声，如下图所示： 彩色噪声严重的图像 这种彩色噪声极大地影响了图像的观感，所以必须去除。而要在手机上运行，算法也一定要轻量化。这里提出一种轻量级而且比较实用的去彩噪的方法：使用Guided Filter去噪。 算法的步骤如下： 将需要去噪的RGB图像(记为I)转换为YUV格式，记为I_YUV； 对I进行guided filter，I自己作为引导图像，得到滤波后的图像，记为I_gf； 将I_gf转换为YUV格式，记为I_gf_YUV； 使用I_YUV的Y通道替换I_gf_YUV的Y通道，得到新的YUV图像，记为I_Y_gf_UV; 将I_Y_gf_UV转换为RGB格式，即为去噪后图像。 去噪后的效果如下： 去除彩色噪声后的图像 可以看到，除了颜色有一点冲淡外，去彩噪的效果相当显著。 测试代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// Win10 + Visual Studio Community 2017 + OpenCV 3.3.0#include \"opencv2/highgui/highgui.hpp\"#include \"opencv2/imgproc/imgproc.hpp\"#include \"opencv2/ximgproc.hpp\"#include &lt;iostream&gt;void chromaNoiseReduction(const cv::Mat&amp; src, cv::Mat&amp; dst, int radius = 30, double eps = 0.03 * 255 * 255)&#123; CV_Assert(src.type() == CV_8UC3); CV_Assert(src.cols % 2 == 0 &amp;&amp; src.rows % 2 == 0); const cv::Mat&amp; I = src; // step 1. convert I to YUV format cv::Mat I_YUV; cv::cvtColor(I, I_YUV, cv::COLOR_BGR2YUV_I420); // step 2. do guided filtering on I cv::Mat I_gf; cv::ximgproc::guidedFilter(I, I, I_gf, radius, eps); // step 3. convert I_gf to YUV format cv::Mat I_gf_YUV; cv::cvtColor(I_gf, I_gf_YUV, cv::COLOR_BGR2YUV_I420); // step 4. replace I_gf_YUV's Y channel with I_YUV's Y channel cv::Mat I_Y_gf_UV = I_gf_YUV.clone(); memcpy(I_Y_gf_UV.data, I_YUV.data, I.cols * I.rows); // step 5. convert I_Y_gf_UV to BGR, which is the result cv::cvtColor(I_Y_gf_UV, dst, cv::COLOR_YUV2BGR_I420);&#125;int main()&#123; cv::Mat image = cv::imread(\"2.jpg\"); if (image.empty()) &#123; std::cout &lt;&lt; \"Couldn't open file.\" &lt;&lt; std::endl; return -1; &#125; cv::Mat result; chromaNoiseReduction(image, result); cv::imwrite(\"result.jpg\", result); return 0;&#125;","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/categories/图像处理/"}],"tags":[{"name":"image-processing","slug":"image-processing","permalink":"http://yoursite.com/tags/image-processing/"},{"name":"denoise","slug":"denoise","permalink":"http://yoursite.com/tags/denoise/"},{"name":"chroma noise","slug":"chroma-noise","permalink":"http://yoursite.com/tags/chroma-noise/"},{"name":"guided filter","slug":"guided-filter","permalink":"http://yoursite.com/tags/guided-filter/"}]},{"title":"分段问题的算法实现","slug":"segment-algorithm","date":"2018-07-19T15:06:29.000Z","updated":"2018-07-19T15:09:14.640Z","comments":true,"path":"2018/07/19/segment-algorithm/","link":"","permalink":"http://yoursite.com/2018/07/19/segment-algorithm/","excerpt":"","text":"分段问题有一个长度为L的数组，其每个元素的索引分别是0，1，2，…，L-1。现欲将其分段，每段的长度为S(S &lt;= L)。求所有段的起始位置的索引。 例如，L为8，S为4时，输出：0，4。 当L不能被S整除时，一般会有如下三种选择： 独立。即不能被整除的部分作为独立的一段。例如，L为9，S为4时，输出：0，4，8。 合并。即不能被整除的部分合并到上一段。例如：L为9，S为4时，输出：0，4。 分条件独立。即当满足一定条件时，不能被整除的部分作为独立的一段，否则合并到上一段。在本文中，我们只考虑一种条件：不能被整除的部分大于S的一半。例如，L为9，S为4时，输出：0，4；而L为11，S为4时，输出：0，4，8。 算法实现独立最直接的实现，就是预先求出所分的段数，然后计算索引：1234567891011121314151617void segment(int L, int S, std::vector&lt;int&gt;&amp; vec)&#123; int count = 0; if (L % S == 0) &#123; count = L / S; &#125; else &#123; count = L / S + 1; &#125; for (int i = 0; i &lt; count; ++i) &#123; vec.push_back(i * S); &#125;&#125; 除此之外，也可以使用迭代的方式，更加简洁：1234567void segment(int L, int S, std::vector&lt;int&gt;&amp; vec)&#123; for (int index = 0; index &lt; L; index += S) &#123; vec.push_back(index); &#125;&#125; 合并同样可以使用两种方式实现：123456789void segment(int L, int S, std::vector&lt;int&gt;&amp; vec)&#123; const int count = L / S; for (int i = 0; i &lt; count; ++i) &#123; vec.push_back(i * S); &#125;&#125; 1234567void segment(int L, int S, std::vector&lt;int&gt;&amp; vec)&#123; for (int index = 0; index &lt;= L - S; index += S) &#123; vec.push_back(index); &#125;&#125; 分条件独立同样可以使用两种方式实现：12345678910111213141516171819202122void segment(int L, int S, std::vector&lt;int&gt;&amp; vec)&#123; int count = 0; if (L % S == 0) &#123; count = L / S; &#125; else &#123; count = L / S; if (L - count * S &gt; S / 2) &#123; count += 1; &#125; &#125; for (int i = 0; i &lt; count; ++i) &#123; vec.push_back(i * S); &#125;&#125; 1234567void segment2(int L, int S, std::vector&lt;int&gt;&amp; vec)&#123; for (int index = 0; index &lt; L - S / 2; index += S) &#123; vec.push_back(index); &#125;&#125; 可以看到，用迭代的方式，代码更加简洁。","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://yoursite.com/tags/algorithm/"},{"name":"segment","slug":"segment","permalink":"http://yoursite.com/tags/segment/"}]},{"title":"基于快速去雾的图像亮度增强方法","slug":"image-brightness-enhancement-based-on-haze-removal","date":"2018-06-09T13:37:09.000Z","updated":"2018-06-09T15:04:32.767Z","comments":true,"path":"2018/06/09/image-brightness-enhancement-based-on-haze-removal/","link":"","permalink":"http://yoursite.com/2018/06/09/image-brightness-enhancement-based-on-haze-removal/","excerpt":"","text":"起因最近在做一个图像处理的算法，因为图像太暗，所以需要对图像的亮度进行增强(不考虑噪声的放大)。尝试了网上搜索到的各种方法后，发现它们存在两个问题：容易造成原本较亮的地方过曝，并且参数不好设置。尝试了一些暗光增强的paper的算法后，发现它们又太慢了。这时我想到曾经看过的一篇paper说过，有一种亮度增强的算法是基于去雾来做的，步骤很简单： 将RGB图像取反(关于图像取反，请参考我的这篇博客OpenCV图像取反)； 对取反后的图像进行去雾； 将去雾后的图像取反。 其背后的理念是：暗光图像取反后，原本接近黑色的像素就会变成接近白色，整张图像就会类似于有雾的图像。于是对这样的图像进行去雾后，白色的像素就变暗了，再反色后，像素就变亮了！没毛病！ 快速去雾算法说起去雾，恐怕大多数人的第一反应就是鼎鼎大名的何恺明博士的暗通道先验去雾算法。我的第一反应也是这个。但是在了解之后，我发现这种方法速度太慢了，难以应用到我的算法中。于是我开始搜索快速的去雾算法，很快找到了这篇论文《基于单幅图像的快速去雾算法》刘倩, 陈茂银, 周东华(这篇文章中也提到了暗通道先验去雾算法的速度太慢)，速度很快，只有O(1)复杂度。 这篇论文我并没有仔细看，而是秉承“拿来主义”的精神，直接根据论文提供的算法流程实现了代码。原因是它的算法流程太简单了，在轻松地实现了代码之后，就没有再看的欲望了~这里贴一下它的算法流程，你们自己看： 算法流程 源码解释我把实现的代码放到了GitHub：IBEABFHR(原谅我这个取名废)，请点进去看效果图，我这里就不重复放了。我感觉效果还是很好的，暗处的亮度增强得很好，亮处虽然有过曝，但并不是很明显。而且控制亮度的参数很好调整，只要随便找一张图像调整好参数，就可以应用于所有图片了。 运行速度代码是用OpenCV实现的，同时支持彩色图像和灰度图像。在我的电脑上(CPU: E3-1230 v3)测试，运行100次取平均值，速度如下： 分辨率 类型 时间 1024x768 灰度图像 8.77ms 1024x768 彩色图像 16.24ms 1920x1080 灰度图像 22.61ms 1920x1080 彩色图像 40.60ms 4160x2340 灰度图像 104.57ms 4160x2340 彩色图像 186.14ms 但是如果你只使用我的算法一次，可能速度要慢得多，原因是第一次取反操作因为未知原因耗费了额外的时间。关于这点，请参看我的这篇博客OpenCV图像取反。 参数调整这个算法总共有两个可变参数。一个是在step 3中进行均值滤波时的所用的滤波半径radius，另一个是在step 5中用的ρ。 radius参数在某些图像上可以控制对比度，数值越大，对比度越强，但在某些图像上不起作用。这个参数取值不能太小，否则增强后的图像会出现光晕。一般不应小于50或者图像宽度和高度最大值的的1/20。 ρ控制图像增强的亮度，数值越大，增强后的图像越亮。一般的取值范围为[1.0, 2.0]。在我的实现中，我使用了一种简单选择的策略，请参看源代码，仅供参考。 关于参数的效果及设置，我参考了这篇博客一种可实时处理 O(1)复杂度图像去雾算法的实现，在此进行感谢。","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/categories/图像处理/"}],"tags":[{"name":"image-processing","slug":"image-processing","permalink":"http://yoursite.com/tags/image-processing/"}]},{"title":"OpenCV两个Mat相减的隐藏秘密","slug":"opencv-abs-vs-absdiff","date":"2018-06-08T15:01:32.000Z","updated":"2018-06-08T16:04:12.848Z","comments":true,"path":"2018/06/08/opencv-abs-vs-absdiff/","link":"","permalink":"http://yoursite.com/2018/06/08/opencv-abs-vs-absdiff/","excerpt":"","text":"起因今天在看同事写的代码时，发现一个“错误”：他的原意是实现以下功能： 12cv::Mat absDiff;cv::absdiff(mat1, mat2, absDiff); 其中mat1和mat2均为CV_8UC1类型。 但是可能是一时没想起这个函数，于是他写成了这个样子： 1cv::Mat absDiff = cv::abs(mat1 - mat2); 问题于是我认真地告诉他，这样做是错的。假设mat1为[0]，mat2为[255]，那么mat1 - mat2将会得到[0]，因为cv::saturate_cast&lt;uchar&gt;(0 - 255) == 0。则cv::abs([0])自然就是[0]，而他的期望是得到[255]。 并且我写出如下代码证明他是错的： 1cv::Mat diff = mat1 - mat; 结果diff确实是[0]。那么cv::Mat absDiff = cv::abs(diff);肯定就是[0]了。 但是他坚持让我用cv::Mat absDiff = cv::abs(mat1 - mat2);测试。结果。。。。absDiff竟然真的是[255]！我当时就震惊了，同时隐隐有一种感觉：这其中一定隐藏着一个天大的秘密。 秘密于是我进入调试模式，认真观察每一步，终于明白了玄机所在。 为什么我得到了[0]先来分析我的测试代码： 1cv::Mat diff = mat1 - mat; 在我以前的观念里，mat1 - mat是两个cv::Mat互相作用，实际上调用的是cv::subtract()。但是事实上，mat1 - mat2调用的是以下函数： 12345678CV_EXPORTS MatExpr operator - (const Mat&amp; a, const Mat&amp; b);MatExpr operator - (const Mat&amp; a, const Mat&amp; b)&#123; MatExpr e; MatOp_AddEx::makeExpr(e, a, b, 1, -1); return e;&#125; 可以看到，mat1 - mat2实际上是生成了一个MatExpr对象。在生成过程中，并没有进行实际的相减操作，而只是保存了a和b，并记录了它们之间期望进行的操作：相减。实际的相减操作是在类型转换时进行的： 12345678//cv::Mat diff = mat1 - mat; // 在这里调用了operator Mat()MatExpr::operator Mat() const&#123; Mat m; op-&gt;assign(*this, m); return m;&#125; 在assign中最终调用了cv::subtract()。所以mat1 - mat2得到了[0]。 为什么同事得到了[255]再来分析我同事的测试代码： 1cv::Mat absDiff = cv::abs(mat1 - mat2); 从上面我们知道，mat1 - mat2生成了一个MatExpr对象。而cv::abs()调用的是 12345678MatExpr abs(const MatExpr&amp; e)&#123; CV_INSTRUMENT_REGION() MatExpr en; e.op-&gt;abs(e, en); return en;&#125; 这个函数再次生成了一个MatExpr对象，其中记录了操作对象e和期望进行的操作：取绝对值，而并没有进行实际的运算。然后同上面一样，它是在赋值给absDiff时调用operator Mat()进行运算的。神奇的地方来了，它把前面的相减操作与这里的取绝对值操作组合到了一起(而不是依次运算)，最终调用的正是cv::absdiff()！ 于是谜底揭开了，一切的原因都是因为MatExpr这个中间对象实现了延迟运算和操作组合。 感叹我还是太年轻！","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/categories/图像处理/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://yoursite.com/tags/OpenCV/"}]},{"title":"OpenCL优化小技巧：预创建所有Kernel","slug":"opencl-trick-precreate-all-kernels","date":"2018-06-07T15:48:43.000Z","updated":"2018-06-07T16:10:59.518Z","comments":true,"path":"2018/06/07/opencl-trick-precreate-all-kernels/","link":"","permalink":"http://yoursite.com/2018/06/07/opencl-trick-precreate-all-kernels/","excerpt":"","text":"最近做了一些图像处理的算法，跑在高通的开发板上，其中使用了OpenCL进行加速。在此过程中，也总结了几个加速的小技巧。今天就来谈其中一个不太有用的小技巧：预创建所有Kernel。 第一次进行OpenCL加速时，我注意到，创建cl_kernel时，会耗费几毫秒到二十几毫秒的时间。如果算法中需要创建几十个cl_kernel，那花费的时间也有几百毫秒了。这让人很难接受。 后来我又注意到，对于同一个Kernel，只有第一次创建时才会花费那么多时间，后续再次创建(无论前面创建的是否已经释放掉)所花费的时间将会大大减少，几乎可以忽略不计。那么机会就来了：我们可以在初始化时预先创建好所有Kernel，再全部释放掉。然后在实际的处理算法中，和平时一样使用clCreateKernel()和clReleaseKernel()，同时也不用担心创建Kernel所花费的额外时间了。 为什么我说这个小技巧不太有用呢？因为只有使用源码，也就是clCreateProgramWithSource()创建program时，才会出现创建Kernel耗时严重的现象。而一般发布出来的算法都会使用二进制文件，也就是clCreateProgramWithBinary()，在保存为二进制时，一定已经先创建所有Kernel了，耗时问题也就不存在了。 但是在加速未完成之前，我们一般还是会直接使用源码来调试。此时预创建所有Kernel，可以排除掉创建Kernel的时间，使得对算法的最终运行时间估计更准确。所以，这个小技巧还是有一点用的:)。","categories":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/categories/OpenCL/"}],"tags":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/tags/OpenCL/"},{"name":"trick","slug":"trick","permalink":"http://yoursite.com/tags/trick/"}]},{"title":"OpenCL clCreateBuffer占用太多时间","slug":"opencl-clcreatebuffer-takes-long-time","date":"2018-06-07T15:36:14.000Z","updated":"2018-06-09T12:59:42.370Z","comments":true,"path":"2018/06/07/opencl-clcreatebuffer-takes-long-time/","link":"","permalink":"http://yoursite.com/2018/06/07/opencl-clcreatebuffer-takes-long-time/","excerpt":"","text":"最近在做一个图像处理的算法，跑在高通平台上，需要使用OpenCL加速。代码分为三个部分： 初始化 处理图像 释放资源 为了尽可能地减少算法的运行时间，我将一切可以预处理的内容都放到了初始化中，其中就包括了创建buffer。在初始化中，我调用clCreateBuffer()创建了9个buffer，共计约占用600MB内存。然后在处理图像中重复使用这些buffer，最后在释放资源中释放所有buffer。 但是在实际测试后发现，每调用一次clCreateBuffer()，都会花费大约70ms的时间，这样一来，创建所有buffer就花费了约600ms的时间。同样地，在释放这些buffer时，每个也会花费几十毫秒的时间。如此，初始化和释放资源的时间就令人比较难以接受。 经同事提醒，我想到了高通文档《Qualcomm® Snapdragon™ Mobile Platform OpenCL General Programming and Optimization》中提到的ION内存。文档中说，使用ION可以避免内存拷贝。那么对缩短创建buffer的时间会不会也有帮助呢？毕竟在我的印象中，使用new创建一段几百MB的内存也才花费几毫秒的时间。 使用ION内存来创建OpenCL buffer需要cl_qcom_ion_host_ptr扩展(在上述文档中有提到)，其说明及示例代码在OpenCL官网可以查到，为了方便，我直接贴到这里：cl_qcom_ion_host_ptr。 然后我在高通平台上进行了测试。测试结果让人半忧半喜。令人忧的是，使用ION内存来创建OpenCL buffer时，clCreateBuffer()只需要零点几毫秒的时间，但是创建ION内存却需要使用数十毫秒的时间，等于创建buffer的时间转移到了创建ION内存上，最终花费的时间差别不大。令人喜的是，上述现象只发生在第一次调用初始化时。后面再次运行算法，再次调用初始化，创建buffer和创建ION内存的时间便会都变为零点几毫秒。另外，无论是第几次调用释放资源，速度都很快，总共只需要数毫秒。 所以问题并未彻底解决。 由于我对这个现象背后的原理知之不详，所以也只能做到这一步了。如果了解原理的话，是否可以真正地缩短创建OpenCL buffer的时间呢？ 另外我还观察到一个现象。在不使用ION内存时，我使用高通的性能分析工具Snapdragon Profiler观察到，运行算法时系统内存会上升约700MB。但是使用ION内存时，系统内存只会上升200多MB。这可能也跟ION的原理有关吧。 如果有哪位朋友知道上述两个现象的原因，可以发邮件告诉我。多谢！ 注：最后还是决定使用ION内存，因为看到高通的Camera相关代码中就使用了ION内存。并且使用ION内存后，算法实际运行时间未发生发化。","categories":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/categories/OpenCL/"}],"tags":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/tags/OpenCL/"},{"name":"trick","slug":"trick","permalink":"http://yoursite.com/tags/trick/"}]},{"title":"OpenCV图像取反","slug":"opencv-image-invert","date":"2018-06-07T13:24:10.000Z","updated":"2018-06-07T14:30:09.035Z","comments":true,"path":"2018/06/07/opencv-image-invert/","link":"","permalink":"http://yoursite.com/2018/06/07/opencv-image-invert/","excerpt":"","text":"最近在做一个基于去雾的图像亮度增强的算法IBEABFHR，其中用到了图像取反的操作。所谓图像取反，就是将RGB图像的每个像素点(r, g, b)，使用(255 - r, 255 - g, 255 - b)替换。对于灰度图像而言，则是将(g)使用(255 - g)替换。如下图所示： RGB图像 RGB取反图像 灰度图像 灰度取反图像 在OpenCV中要实现此操作，可以遍历每个像素，用255去减，此方法不再赘述。更直接地，可以对图像(cv::Mat)整体做减法： 12cv::Mat image = cv::imread(\"rgb.jpg\");cv::Mat image_inverse = cv::Scalar(255, 255, 255) - image; 也可以使用cv::subtract，效果是一样的: 123cv::Mat image = cv::imread(\"rgb.jpg\");cv::Mat image_inverse;cv::subtract(cv::Scalar(255, 255, 255), image, image_inverse); 但是我在网上搜索到了更好的方法，那就是使用位运算中的取反操作(~)： 12cv::Mat image = cv::imread(\"rgb.jpg\");cv::Mat image_inverse = ~image; 原理是，对于一个unsigned char类型的变量c，255 - c与~c是相等的。 此方法在opencv2\\core\\mat.hpp中声明如下： 1CV_EXPORTS MatExpr operator ~(const Mat&amp; m); 需要注意的是，此方法仅对整数型的cv::Mat有效。 一般来说，位运算的速度都是比较快的，事实也是如此，我使用一张4160x2340的图像来做测试，两种方法各运算100次取平均时间，结果如下: 相减法：14.1862ms 位运算法：11.8158ms 但是在测试过程中发现一个问题：第一次取反操作(无论是相减法还是位运算法)竟然需要耗时100多毫秒，而后面再进行取反操作速度就变正常了，只要10几毫秒。现在还不知道原因是什么。希望有知道的朋友可以告诉我。可以发我邮箱。下面附上出现此问题的代码。 12345678910111213141516171819202122232425262728293031323334353637// Timer.h#pragma once#include &lt;iostream&gt;#include &lt;chrono&gt;class Timer&#123;public: Timer() : t1(res::zero()) , t2(res::zero()) &#123; tic(); &#125; ~Timer() &#123;&#125; void tic() &#123; t1 = clock::now(); &#125; void toc(const char* str) &#123; t2 = clock::now(); std::cout &lt;&lt; str &lt;&lt; \" time: \" &lt;&lt; std::chrono::duration_cast&lt;res&gt;(t2 - t1).count() / 1e3 &lt;&lt; \"ms.\" &lt;&lt; std::endl; &#125;private: typedef std::chrono::high_resolution_clock clock; typedef std::chrono::microseconds res; clock::time_point t1; clock::time_point t2;&#125;; 1234567891011121314151617181920212223242526272829303132333435363738// main.cpp#include &lt;iostream&gt;#include \"opencv2/highgui/highgui.hpp\"#include \"opencv2/imgproc/imgproc.hpp\"#include \"Timer.h\"int main()&#123; cv::Mat image = cv::imread(\"5.jpg\"); if (image.empty()) &#123; std::cout &lt;&lt; \"Couldn't open file.\" &lt;&lt; std::endl; system(\"pause\"); return -1; &#125; Timer timer; timer.tic(); cv::Mat temp = cv::Scalar(255, 255, 255) - image; timer.toc(\"single \"); timer.tic(); for (int i = 0; i &lt; 100; ++i) &#123; cv::Mat temp1 = cv::Scalar(255, 255, 255) - image; &#125; timer.toc(\"subtract\"); timer.tic(); for (int i = 0; i &lt; 100; ++i) &#123; cv::Mat temp1 = ~image; &#125; timer.toc(\"operator~\"); system(\"pause\"); return 0;&#125; 注：本文使用OpenCV 3.3.0。","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/categories/图像处理/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://yoursite.com/tags/OpenCV/"}]},{"title":"C++11高精度计时器","slug":"cpp-timer","date":"2018-06-05T14:26:41.000Z","updated":"2018-06-05T15:07:31.163Z","comments":true,"path":"2018/06/05/cpp-timer/","link":"","permalink":"http://yoursite.com/2018/06/05/cpp-timer/","excerpt":"","text":"做图像处理算法时，免不了要测量函数的运行时间。以前我都是使用OpenCV的计时函数cv::getTickCount()和cv::getTickFrequency()，但是这样一来，在不使用OpenCV的项目中就没法用了。幸好C++11增加了std::chrono库，可以很方便地实现跨平台的时间测量。于是我封装了一个简单的计时器类，这样只要将其简单地添加到项目中，就可以直接使用了。此计时器单位为毫秒，但可以精确到微秒级。 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;chrono&gt;class Timer&#123;public: Timer() : t1(res::zero()) , t2(res::zero()) &#123; tic(); &#125; ~Timer() &#123;&#125; void tic() &#123; t1 = clock::now(); &#125; void toc(const char* str) &#123; t2 = clock::now(); std::cout &lt;&lt; str &lt;&lt; \" time: \" &lt;&lt; std::chrono::duration_cast&lt;res&gt;(t2 - t1).count() / 1e3 &lt;&lt; \"ms.\" &lt;&lt; std::endl; &#125;private: typedef std::chrono::high_resolution_clock clock; typedef std::chrono::microseconds res; clock::time_point t1; clock::time_point t2;&#125;; 测试代码如下： 1234567891011121314int main()&#123; Timer timer; std::cout &lt;&lt; \"1\" &lt;&lt; std::endl; timer.toc(\"output 1\"); timer.tic(); std::cout &lt;&lt; \"2\" &lt;&lt; std::endl; timer.toc(\"output 2\"); system(\"pause\"); return 0;&#125; 输出如下： 123451output 1 time: 0.26ms.2output 2 time: 0.039ms.请按任意键继续. . .","categories":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"提取图像细节的两种方法","slug":"two-ways-of-extracting-detail-of-image","date":"2018-04-29T13:50:14.000Z","updated":"2018-06-05T15:25:29.169Z","comments":true,"path":"2018/04/29/two-ways-of-extracting-detail-of-image/","link":"","permalink":"http://yoursite.com/2018/04/29/two-ways-of-extracting-detail-of-image/","excerpt":"","text":"一幅图像可以分解为两层：底层(base layer)和细节层(detail layer)。底层包含图像的低频信息，反映了图像在大尺度上的强度变化；细节层包含图像的高频信息，反映了图像在小尺度上的细节。分解图像有两种方式，以下分别进行解释。 1. 加性分解要获取图像的底层，即图像的低频信息，使用低通滤波(如均值滤波(mean filter)，高斯滤波(gaussian filter)，导向滤波(guided filter))对图像进行滤波即可：$$B = f(I) $$其中$I$表示要分解的图像，$f(\\cdot)$表示低通滤波操作，$B$为提取的底层。 提取底层后，使用源图像减去底层，即为细节层：$$D = I - B$$其中$D$表示提取的细节层。 因为底层加上细节层即为源图像，所以我称此种分解方法为加性分解，对应于加性噪声。关于此种方法的应用，可以参见[1]。 2. 乘性分解获取底层的方法与加性分解相同。然后使用源图像除以底层，即可得到细节层：$$D = \\frac{I + \\epsilon}{B + \\epsilon}$$其中$\\epsilon$为一个很小的常数，以防止除零错误。 因为底层乘以细节层即为源图像，所以我称此种分解方法为乘性分解，对应于乘性噪声。关于此种方法的应用，可以参见[2]。在其他文章中，此处得到的细节层也称为商图像(quotient image)[3]或比例图像(ratio image)[4]。 3. 代码及效果123456789101112131415161718192021222324252627282930313233343536373839404142434445// 图像细节提取。// 编程环境：Visual Studio Community 2015 + OpenCV 3.3.0#include \"opencv2/core/core.hpp\"#include \"opencv2/imgcodecs/imgcodecs.hpp\"#include \"opencv2/imgproc/imgproc.hpp\"#include \"opencv2/highgui/highgui.hpp\"int main()&#123; cv::Mat I = cv::imread(\"im.png\"); if (I.empty()) &#123; return -1; &#125; I.convertTo(I, CV_32FC3); cv::Mat B; cv::boxFilter(I, B, -1, cv::Size(31, 31)); // 1. 加性分解 cv::Mat D1 = I - B; // 2. 乘性分解 const float epsilon = 1.0f; cv::Mat D2 = (I + epsilon) / (B + epsilon); // 显示图像 I.convertTo(I, CV_8UC3); cv::imshow(\"源图像\", I); B.convertTo(B, CV_8UC3); cv::imshow(\"Base layer\", B); D1 = cv::abs(D1); // 因为包含负数，所以取绝对值 D1.convertTo(D1, CV_8UC3); cv::imshow(\"Detail layer 1\", D1); cv::normalize(D2, D2, 0.0, 255.0, cv::NORM_MINMAX); // 归一化 D2.convertTo(D2, CV_8UC3); cv::imshow(\"Detail layer 2\", D2); cv::waitKey(); return 0;&#125; 图1：源图像 图2：Base Layer 图3：Detail Layer1 图4：Detail Layer2 4. 应用提取图像的细节层后，可以进行细节增强(detail enhancement)或细节转移(detail transfer)[2]等。 5. 参考文献[1] S. Li, X. Kang, and J. Hu. Image fusion with guided fltering. IEEE Transactions on Image Processing, 22(7):2864–2875, July 2013. [2] Georg Petschnigg, Richard Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, and Kentaro Toyama. Digital photography with ﬂash and no-ﬂash image pairs. In ACM transactions on graphics (TOG), volume 23, pages 664–672. ACM, 2004. [3] Amnon Shashua and Tammy Riklin-Raviv. The quotient image: Class-based re-rendering and recognition with varying illuminations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(2):129–139, 2001. [4] Zicheng Liu, Ying Shan, and Zhengyou Zhang. Expressive expression mapping with ratio images. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 271–276. ACM, 2001.","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/categories/图像处理/"}],"tags":[{"name":"image-processing","slug":"image-processing","permalink":"http://yoursite.com/tags/image-processing/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-04-28T15:23:59.783Z","updated":"2018-04-28T14:34:14.543Z","comments":true,"path":"2018/04/28/hello-world/","link":"","permalink":"http://yoursite.com/2018/04/28/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"test","slug":"test","permalink":"http://yoursite.com/tags/test/"}]}]}