{"meta":{"title":"天律界","subtitle":"精诚所至，金石为开","description":"天律界中子的Github page","author":"天律界中子","url":"http://yoursite.com"},"pages":[{"title":"","date":"2018-04-29T13:47:05.629Z","updated":"2018-04-29T13:47:05.629Z","comments":true,"path":"About/index.html","permalink":"http://yoursite.com/About/index.html","excerpt":"","text":"简介图像处理算法工程师一枚，以“天律界中子”的昵称混迹于网络江湖。 教育经历 2009-2013 南昌大学"},{"title":"","date":"2018-04-28T14:24:22.605Z","updated":"2018-04-28T14:24:22.605Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2018-04-28T14:23:30.645Z","updated":"2018-04-28T14:23:30.645Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"OpenCL优化小技巧：预创建所有Kernel","slug":"opencl-trick-precreate-all-kernels","date":"2018-06-07T15:48:43.000Z","updated":"2018-06-07T16:10:59.518Z","comments":true,"path":"2018/06/07/opencl-trick-precreate-all-kernels/","link":"","permalink":"http://yoursite.com/2018/06/07/opencl-trick-precreate-all-kernels/","excerpt":"","text":"最近做了一些图像处理的算法，跑在高通的开发板上，其中使用了OpenCL进行加速。在此过程中，也总结了几个加速的小技巧。今天就来谈其中一个不太有用的小技巧：预创建所有Kernel。 第一次进行OpenCL加速时，我注意到，创建cl_kernel时，会耗费几毫秒到二十几毫秒的时间。如果算法中需要创建几十个cl_kernel，那花费的时间也有几百毫秒了。这让人很难接受。 后来我又注意到，对于同一个Kernel，只有第一次创建时才会花费那么多时间，后续再次创建(无论前面创建的是否已经释放掉)所花费的时间将会大大减少，几乎可以忽略不计。那么机会就来了：我们可以在初始化时预先创建好所有Kernel，再全部释放掉。然后在实际的处理算法中，和平时一样使用clCreateKernel()和clReleaseKernel()，同时也不用担心创建Kernel所花费的额外时间了。 为什么我说这个小技巧不太有用呢？因为只有使用源码，也就是clCreateProgramWithSource()创建program时，才会出现创建Kernel耗时严重的现象。而一般发布出来的算法都会使用二进制文件，也就是clCreateProgramWithBinary()，在保存为二进制时，一定已经先创建所有Kernel了，耗时问题也就不存在了。 但是在加速未完成之前，我们一般还是会直接使用源码来调试。此时预创建所有Kernel，可以排除掉创建Kernel的时间，使得对算法的最终运行时间估计更准确。所以，这个小技巧还是有一点用的:)。","categories":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/categories/OpenCL/"}],"tags":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/tags/OpenCL/"},{"name":"trick","slug":"trick","permalink":"http://yoursite.com/tags/trick/"}]},{"title":"OpenCL clCreateBuffer占用太多时间","slug":"opencl-clcreatebuffer-takes-long-time","date":"2018-06-07T15:36:14.000Z","updated":"2018-06-07T15:36:13.677Z","comments":true,"path":"2018/06/07/opencl-clcreatebuffer-takes-long-time/","link":"","permalink":"http://yoursite.com/2018/06/07/opencl-clcreatebuffer-takes-long-time/","excerpt":"","text":"最近在做一个图像处理的算法，跑在高通平台上，需要使用OpenCL加速。代码分为三个部分： 初始化 处理图像 释放资源 为了尽可能地减少算法的运行时间，我将一切可以预处理的内容都放到了初始化中，其中就包括了创建buffer。在初始化中，我调用clCreateBuffer()创建了9个buffer，共计约占用600MB内存。然后在处理图像中重复使用这些buffer，最后在释放资源中释放所有buffer。 但是在实际测试后发现，每调用一次clCreateBuffer()，都会花费大约70ms的时间，这样一来，创建所有buffer就花费了约600ms的时间。同样地，在释放这些buffer时，每个也会花费几十毫秒的时间。如此，初始化和释放资源的时间就令人比较难以接受。 经同事提醒，我想到了高通文档《Qualcomm® Snapdragon™ Mobile Platform OpenCL General Programming and Optimization》中提到的ION内存。文档中说，使用ION可以避免内存拷贝。但是对缩短创建buffer的时间会不会有帮助呢？毕竟在我的印象中，使用new创建一段几百MB的内存也才花费几毫秒的时间。 使用ION内存来创建OpenCL buffer需要cl_qcom_ion_host_ptr扩展(在上述文档中有提到)，其说明及示例代码在OpenCL官网可以查到，为了方便，我直接贴到这里：cl_qcom_ion_host_ptr。 然后我在高通平台上进行了测试。测试结果让人半忧半喜。令人忧的是，使用ION内存来创建OpenCL buffer时，clCreateBuffer()只需要几毫秒的时间，但是创建ION内存却需要使用数十毫秒的时间，等于创建buffer的时间转移到了创建ION内存上，最终花费的时间差别不大。令人喜的是，上述现象只发生在第一次调用初始化时。后面再次运行算法，再次调用初始化，创建buffer和创建ION内存的时间便会都变为几毫秒。另外，无论是第几次调用释放资源，速度都很快，只需要数毫秒。 我只是观察到了这个现象，对于其背后的原理却是知之不详。如果了解原理的话，是否可以真正地缩短创建OpenCL buffer的时间呢？ 另外我还观察到一个现象。在不使用ION内存时，我使用高通的性能分析工具Snapdragon Profiler观察到，运行算法时系统内存会上升约700MB。但是使用ION内存时，系统内存只会上升200多MB。这可能也跟ION的原理有关吧。 如果有哪位朋友知道上述两个现象的原因，可以发邮件告诉我。多谢！","categories":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/categories/OpenCL/"}],"tags":[{"name":"OpenCL","slug":"OpenCL","permalink":"http://yoursite.com/tags/OpenCL/"},{"name":"trick","slug":"trick","permalink":"http://yoursite.com/tags/trick/"}]},{"title":"OpenCV图像取反","slug":"opencv-image-invert","date":"2018-06-07T13:24:10.000Z","updated":"2018-06-07T14:30:09.035Z","comments":true,"path":"2018/06/07/opencv-image-invert/","link":"","permalink":"http://yoursite.com/2018/06/07/opencv-image-invert/","excerpt":"","text":"最近在做一个基于去雾的图像亮度增强的算法IBEABFHR，其中用到了图像取反的操作。所谓图像取反，就是将RGB图像的每个像素点(r, g, b)，使用(255 - r, 255 - g, 255 - b)替换。对于灰度图像而言，则是将(g)使用(255 - g)替换。如下图所示： RGB图像 RGB取反图像 灰度图像 灰度取反图像 在OpenCV中要实现此操作，可以遍历每个像素，用255去减，此方法不再赘述。更直接地，可以对图像(cv::Mat)整体做减法： 12cv::Mat image = cv::imread(\"rgb.jpg\");cv::Mat image_inverse = cv::Scalar(255, 255, 255) - image; 也可以使用cv::subtract，效果是一样的: 123cv::Mat image = cv::imread(\"rgb.jpg\");cv::Mat image_inverse;cv::subtract(cv::Scalar(255, 255, 255), image, image_inverse); 但是我在网上搜索到了更好的方法，那就是使用位运算中的取反操作(~)： 12cv::Mat image = cv::imread(\"rgb.jpg\");cv::Mat image_inverse = ~image; 原理是，对于一个unsigned char类型的变量c，255 - c与~c是相等的。 此方法在opencv2\\core\\mat.hpp中声明如下： 1CV_EXPORTS MatExpr operator ~(const Mat&amp; m); 需要注意的是，此方法仅对整数型的cv::Mat有效。 一般来说，位运算的速度都是比较快的，事实也是如此，我使用一张4160x2340的图像来做测试，两种方法各运算100次取平均时间，结果如下: 相减法：14.1862ms 位运算法：11.8158ms 但是在测试过程中发现一个问题：第一次取反操作(无论是相减法还是位运算法)竟然需要耗时100多毫秒，而后面再进行取反操作速度就变正常了，只要10几毫秒。现在还不知道原因是什么。希望有知道的朋友可以告诉我。可以发我邮箱。下面附上出现此问题的代码。 12345678910111213141516171819202122232425262728293031323334353637// Timer.h#pragma once#include &lt;iostream&gt;#include &lt;chrono&gt;class Timer&#123;public: Timer() : t1(res::zero()) , t2(res::zero()) &#123; tic(); &#125; ~Timer() &#123;&#125; void tic() &#123; t1 = clock::now(); &#125; void toc(const char* str) &#123; t2 = clock::now(); std::cout &lt;&lt; str &lt;&lt; \" time: \" &lt;&lt; std::chrono::duration_cast&lt;res&gt;(t2 - t1).count() / 1e3 &lt;&lt; \"ms.\" &lt;&lt; std::endl; &#125;private: typedef std::chrono::high_resolution_clock clock; typedef std::chrono::microseconds res; clock::time_point t1; clock::time_point t2;&#125;; 1234567891011121314151617181920212223242526272829303132333435363738// main.cpp#include &lt;iostream&gt;#include \"opencv2/highgui/highgui.hpp\"#include \"opencv2/imgproc/imgproc.hpp\"#include \"Timer.h\"int main()&#123; cv::Mat image = cv::imread(\"5.jpg\"); if (image.empty()) &#123; std::cout &lt;&lt; \"Couldn't open file.\" &lt;&lt; std::endl; system(\"pause\"); return -1; &#125; Timer timer; timer.tic(); cv::Mat temp = cv::Scalar(255, 255, 255) - image; timer.toc(\"single \"); timer.tic(); for (int i = 0; i &lt; 100; ++i) &#123; cv::Mat temp1 = cv::Scalar(255, 255, 255) - image; &#125; timer.toc(\"subtract\"); timer.tic(); for (int i = 0; i &lt; 100; ++i) &#123; cv::Mat temp1 = ~image; &#125; timer.toc(\"operator~\"); system(\"pause\"); return 0;&#125; 注：本文使用OpenCV 3.3.0。","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/categories/图像处理/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://yoursite.com/tags/OpenCV/"}]},{"title":"C++11高精度计时器","slug":"cpp-timer","date":"2018-06-05T14:26:41.000Z","updated":"2018-06-05T15:07:31.163Z","comments":true,"path":"2018/06/05/cpp-timer/","link":"","permalink":"http://yoursite.com/2018/06/05/cpp-timer/","excerpt":"","text":"做图像处理算法时，免不了要测量函数的运行时间。以前我都是使用OpenCV的计时函数cv::getTickCount()和cv::getTickFrequency()，但是这样一来，在不使用OpenCV的项目中就没法用了。幸好C++11增加了std::chrono库，可以很方便地实现跨平台的时间测量。于是我封装了一个简单的计时器类，这样只要将其简单地添加到项目中，就可以直接使用了。此计时器单位为毫秒，但可以精确到微秒级。 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;chrono&gt;class Timer&#123;public: Timer() : t1(res::zero()) , t2(res::zero()) &#123; tic(); &#125; ~Timer() &#123;&#125; void tic() &#123; t1 = clock::now(); &#125; void toc(const char* str) &#123; t2 = clock::now(); std::cout &lt;&lt; str &lt;&lt; \" time: \" &lt;&lt; std::chrono::duration_cast&lt;res&gt;(t2 - t1).count() / 1e3 &lt;&lt; \"ms.\" &lt;&lt; std::endl; &#125;private: typedef std::chrono::high_resolution_clock clock; typedef std::chrono::microseconds res; clock::time_point t1; clock::time_point t2;&#125;; 测试代码如下： 1234567891011121314int main()&#123; Timer timer; std::cout &lt;&lt; \"1\" &lt;&lt; std::endl; timer.toc(\"output 1\"); timer.tic(); std::cout &lt;&lt; \"2\" &lt;&lt; std::endl; timer.toc(\"output 2\"); system(\"pause\"); return 0;&#125; 输出如下： 123451output 1 time: 0.26ms.2output 2 time: 0.039ms.请按任意键继续. . .","categories":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"提取图像细节的两种方法","slug":"two-ways-of-extracting-detail-of-image","date":"2018-04-29T13:50:14.000Z","updated":"2018-06-05T15:25:29.169Z","comments":true,"path":"2018/04/29/two-ways-of-extracting-detail-of-image/","link":"","permalink":"http://yoursite.com/2018/04/29/two-ways-of-extracting-detail-of-image/","excerpt":"","text":"一幅图像可以分解为两层：底层(base layer)和细节层(detail layer)。底层包含图像的低频信息，反映了图像在大尺度上的强度变化；细节层包含图像的高频信息，反映了图像在小尺度上的细节。分解图像有两种方式，以下分别进行解释。 1. 加性分解要获取图像的底层，即图像的低频信息，使用低通滤波(如均值滤波(mean filter)，高斯滤波(gaussian filter)，导向滤波(guided filter))对图像进行滤波即可：$$B = f(I) $$其中$I$表示要分解的图像，$f(\\cdot)$表示低通滤波操作，$B$为提取的底层。 提取底层后，使用源图像减去底层，即为细节层：$$D = I - B$$其中$D$表示提取的细节层。 因为底层加上细节层即为源图像，所以我称此种分解方法为加性分解，对应于加性噪声。关于此种方法的应用，可以参见[1]。 2. 乘性分解获取底层的方法与加性分解相同。然后使用源图像除以底层，即可得到细节层：$$D = \\frac{I + \\epsilon}{B + \\epsilon}$$其中$\\epsilon$为一个很小的常数，以防止除零错误。 因为底层乘以细节层即为源图像，所以我称此种分解方法为乘性分解，对应于乘性噪声。关于此种方法的应用，可以参见[2]。在其他文章中，此处得到的细节层也称为商图像(quotient image)[3]或比例图像(ratio image)[4]。 3. 代码及效果123456789101112131415161718192021222324252627282930313233343536373839404142434445// 图像细节提取。// 编程环境：Visual Studio Community 2015 + OpenCV 3.3.0#include \"opencv2/core/core.hpp\"#include \"opencv2/imgcodecs/imgcodecs.hpp\"#include \"opencv2/imgproc/imgproc.hpp\"#include \"opencv2/highgui/highgui.hpp\"int main()&#123; cv::Mat I = cv::imread(\"im.png\"); if (I.empty()) &#123; return -1; &#125; I.convertTo(I, CV_32FC3); cv::Mat B; cv::boxFilter(I, B, -1, cv::Size(31, 31)); // 1. 加性分解 cv::Mat D1 = I - B; // 2. 乘性分解 const float epsilon = 1.0f; cv::Mat D2 = (I + epsilon) / (B + epsilon); // 显示图像 I.convertTo(I, CV_8UC3); cv::imshow(\"源图像\", I); B.convertTo(B, CV_8UC3); cv::imshow(\"Base layer\", B); D1 = cv::abs(D1); // 因为包含负数，所以取绝对值 D1.convertTo(D1, CV_8UC3); cv::imshow(\"Detail layer 1\", D1); cv::normalize(D2, D2, 0.0, 255.0, cv::NORM_MINMAX); // 归一化 D2.convertTo(D2, CV_8UC3); cv::imshow(\"Detail layer 2\", D2); cv::waitKey(); return 0;&#125; 图1：源图像 图2：Base Layer 图3：Detail Layer1 图4：Detail Layer2 4. 应用提取图像的细节层后，可以进行细节增强(detail enhancement)或细节转移(detail transfer)[2]等。 5. 参考文献[1] S. Li, X. Kang, and J. Hu. Image fusion with guided fltering. IEEE Transactions on Image Processing, 22(7):2864–2875, July 2013. [2] Georg Petschnigg, Richard Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, and Kentaro Toyama. Digital photography with ﬂash and no-ﬂash image pairs. In ACM transactions on graphics (TOG), volume 23, pages 664–672. ACM, 2004. [3] Amnon Shashua and Tammy Riklin-Raviv. The quotient image: Class-based re-rendering and recognition with varying illuminations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(2):129–139, 2001. [4] Zicheng Liu, Ying Shan, and Zhengyou Zhang. Expressive expression mapping with ratio images. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 271–276. ACM, 2001.","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://yoursite.com/categories/图像处理/"}],"tags":[{"name":"image-processing","slug":"image-processing","permalink":"http://yoursite.com/tags/image-processing/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-04-28T15:23:59.783Z","updated":"2018-04-28T14:34:14.543Z","comments":true,"path":"2018/04/28/hello-world/","link":"","permalink":"http://yoursite.com/2018/04/28/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"test","slug":"test","permalink":"http://yoursite.com/tags/test/"}]}]}